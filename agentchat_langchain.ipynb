{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2b803c17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b803c17",
        "outputId": "2e12aa3f-e46c-4b82-cc2e-1495f70a2961"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: celery in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (5.3.6)\n",
            "Requirement already satisfied: billiard<5.0,>=4.2.0 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (4.2.0)\n",
            "Requirement already satisfied: click-didyoumean>=0.3.0 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (0.3.0)\n",
            "Requirement already satisfied: click-plugins>=1.1.1 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (1.1.1)\n",
            "Requirement already satisfied: click-repl>=0.2.0 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (0.3.0)\n",
            "Requirement already satisfied: click<9.0,>=8.1.2 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (8.1.7)\n",
            "Requirement already satisfied: kombu<6.0,>=5.3.4 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (5.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (2023.3)\n",
            "Requirement already satisfied: vine<6.0,>=5.1.0 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from celery) (5.1.0)\n",
            "Requirement already satisfied: prompt-toolkit>=3.0.36 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from click-repl>=0.2.0->celery) (3.0.43)\n",
            "Requirement already satisfied: amqp<6.0.0,>=5.1.1 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from kombu<6.0,>=5.3.4->celery) (5.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->celery) (1.16.0)\n",
            "Requirement already satisfied: wcwidth in /home/ameya/Documents/model_host/autogen/.venv/lib/python3.11/site-packages (from prompt-toolkit>=3.0.36->click-repl>=0.2.0->celery) (0.2.12)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install celery"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "dca301a4",
      "metadata": {
        "id": "dca301a4"
      },
      "outputs": [],
      "source": [
        "import autogen\n",
        "\n",
        "config_list = autogen.config_list_from_json(\n",
        "    \"OAI_CONFIG_LIST\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "qCzNbbVajvpc",
      "metadata": {
        "id": "qCzNbbVajvpc"
      },
      "outputs": [],
      "source": [
        "# Import things that are needed generically\n",
        "from langchain.pydantic_v1 import BaseModel, Field\n",
        "from langchain.tools import BaseTool\n",
        "from typing import Optional, Type\n",
        "import math\n",
        "import os\n",
        "\n",
        "class CircumferenceToolInput(BaseModel):\n",
        "    radius: float = Field()\n",
        "\n",
        "class CircumferenceTool(BaseTool):\n",
        "    name = \"circumference_calculator\"\n",
        "    description = \"Use this tool when you need to calculate a circumference using the radius of a circle\"\n",
        "    args_schema: Type[BaseModel] = CircumferenceToolInput\n",
        "\n",
        "    def _run(self, radius: float):\n",
        "        return float(radius) * 2.0 * math.pi\n",
        "    \n",
        "def get_file_path_of_example():\n",
        "    # Get the current working directory\n",
        "    current_dir = os.getcwd()\n",
        "\n",
        "    # Go one directory up\n",
        "    parent_dir = os.path.dirname(current_dir)\n",
        "\n",
        "    # Move to the target directory\n",
        "    target_folder = os.path.join(parent_dir, \"test\")\n",
        "\n",
        "    # Construct the path to your target file\n",
        "    file_path = os.path.join(target_folder, \"test_files/radius.txt\")\n",
        "    \n",
        "    return target_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "COlL5_98atDs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COlL5_98atDs",
        "outputId": "24ce236d-8993-4a69-99e2-65453574d61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "Go through the folder with the path /home/ameya/Documents/model_host/autogen/test, and plot a Sinusoidal graph .\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
            "\n",
            "\u001b[32m***** Suggested function Call: read_file *****\u001b[0m\n",
            "Arguments: \n",
            "{\n",
            "  \"file_path\": \"/home/ameya/Documents/model_host/autogen/test/graph.py\"\n",
            "}\n",
            "\u001b[32m**********************************************\u001b[0m\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[35m\n",
            ">>>>>>>> EXECUTING FUNCTION read_file...\u001b[0m\n",
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "\u001b[32m***** Response from calling function \"read_file\" *****\u001b[0m\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Enable inline plotting for Jupyter notebook\n",
            "%matplotlib inline\n",
            "\n",
            "# Generate sample data\n",
            "x = np.linspace(0, 10, 100)\n",
            "y = np.sin(x)\n",
            "\n",
            "# Plot the data\n",
            "plt.plot(x, y)\n",
            "\n",
            "# Add labels and title\n",
            "plt.xlabel('X-axis')\n",
            "plt.ylabel('Y-axis')\n",
            "plt.title('Simple Sinusoidal Plot')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n",
            "\n",
            "\u001b[32m******************************************************\u001b[0m\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
            "\n",
            "Here is the code to plot a Sinusoidal graph:\n",
            "\n",
            "```python\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Enable inline plotting for Jupyter notebook\n",
            "%matplotlib inline\n",
            "\n",
            "# Generate sample data\n",
            "x = np.linspace(0, 10, 100)\n",
            "y = np.sin(x)\n",
            "\n",
            "# Plot the data\n",
            "plt.plot(x, y)\n",
            "\n",
            "# Add labels and title\n",
            "plt.xlabel('X-axis')\n",
            "plt.ylabel('Y-axis')\n",
            "plt.title('Simple Sinusoidal Plot')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n",
            "```\n",
            "\n",
            "Please let me know if you need any further assistance.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[31m\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "exitcode: 1 (execution failed)\n",
            "Code output: \n",
            "  File \"\", line 5\n",
            "    %matplotlib inline\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33mchatbot\u001b[0m (to user_proxy):\n",
            "\n",
            "It seems that the code is intended to be run in a Jupyter notebook environment, where the `%matplotlib inline` magic command is used to display the plot inline. However, executing this code directly in a script or terminal will result in a `SyntaxError` because the `%matplotlib inline` command is specific to Jupyter notebooks.\n",
            "\n",
            "To plot a Sinusoidal graph in a script or terminal, you can modify the code as follows:\n",
            "\n",
            "```python\n",
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "# Generate sample data\n",
            "x = np.linspace(0, 10, 100)\n",
            "y = np.sin(x)\n",
            "\n",
            "# Plot the data\n",
            "plt.plot(x, y)\n",
            "\n",
            "# Add labels and title\n",
            "plt.xlabel('X-axis')\n",
            "plt.ylabel('Y-axis')\n",
            "plt.title('Simple Sinusoidal Plot')\n",
            "\n",
            "# Show the plot\n",
            "plt.show()\n",
            "```\n",
            "\n",
            "Please try running the modified code and let me know if you encounter any issues.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[31m\n",
            ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "execute_code was called without specifying a value for use_docker. Since the python docker package is not available, code will be run natively. Note: this fallback behavior is subject to change\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to chatbot):\n",
            "\n",
            "exitcode: 0 (execution succeeded)\n",
            "Code output: \n",
            "Figure(640x480)\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-AeXDmWaRtHFPXo4H19CNhKkN on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(data)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m chatbot \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mAssistantAgent(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mchatbot\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     system_message\u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReply TERMINATE when the task is done. The string \u001b[39m\u001b[39m{\u001b[39;00mfile_metadata(metadata)\u001b[39m}\u001b[39;00m\u001b[39m has details of all the files and their description.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     llm_config\u001b[39m=\u001b[39mllm_config,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m )\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     chatbot,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGo through the folder with the path \u001b[39;49m\u001b[39m{\u001b[39;49;00mget_file_path_of_example()\u001b[39m}\u001b[39;49;00m\u001b[39m, and plot a Sinusoidal graph .\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m#7.81mm in the file\u001b[39;49;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m     llm_config\u001b[39m=\u001b[39;49mllm_config,\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ameya/Documents/model_host/autogen/notebook/agentchat_langchain.ipynb#W3sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:556\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 556\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:489\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    487\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:489\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    487\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
            "    \u001b[0;31m[... skipping similar frames: ConversableAgent.send at line 354 (4 times), ConversableAgent.receive at line 489 (3 times)]\u001b[0m\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:489\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    487\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:487\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:962\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 962\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    964\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:631\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    628\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    630\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    632\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages\n\u001b[1;32m    633\u001b[0m )\n\u001b[1;32m    635\u001b[0m \u001b[39m# TODO: line 301, line 271 is converting messages to dict. Can be removed after ChatCompletionMessage_to_dict is merged.\u001b[39;00m\n\u001b[1;32m    636\u001b[0m extracted_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mextract_text_or_completion_object(response)[\u001b[39m0\u001b[39m]\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/oai/client.py:260\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[39mcontinue\u001b[39;00m  \u001b[39m# filter is not passed; try the next config\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 260\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_completions_create(client, params)\n\u001b[1;32m    261\u001b[0m \u001b[39mexcept\u001b[39;00m APIError \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    262\u001b[0m     error_code \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(err, \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/autogen/oai/client.py:346\u001b[0m, in \u001b[0;36mOpenAIWrapper._completions_create\u001b[0;34m(self, client, params)\u001b[0m\n\u001b[1;32m    344\u001b[0m     params \u001b[39m=\u001b[39m params\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    345\u001b[0m     params[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     response \u001b[39m=\u001b[39m completions\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    347\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_utils/_utils.py:270\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    269\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 270\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/resources/chat/completions.py:645\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    597\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    598\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    643\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    644\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 645\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    646\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    647\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    648\u001b[0m             {\n\u001b[1;32m    649\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    650\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    651\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    652\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    653\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    654\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    655\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: logprobs,\n\u001b[1;32m    656\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    657\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    658\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    659\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    660\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    661\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    662\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    663\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    664\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    665\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    666\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_logprobs\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_logprobs,\n\u001b[1;32m    667\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    668\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    669\u001b[0m             },\n\u001b[1;32m    670\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    671\u001b[0m         ),\n\u001b[1;32m    672\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    673\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    674\u001b[0m         ),\n\u001b[1;32m    675\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    676\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    677\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    678\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:1088\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1075\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1076\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1083\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1084\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1085\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1086\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1087\u001b[0m     )\n\u001b[0;32m-> 1088\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:853\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    845\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    846\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    851\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    852\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    854\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    855\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    856\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    857\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    858\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    859\u001b[0m     )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    917\u001b[0m         options,\n\u001b[1;32m    918\u001b[0m         cast_to,\n\u001b[1;32m    919\u001b[0m         retries,\n\u001b[1;32m    920\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    921\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    922\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    961\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    962\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    963\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    964\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:916\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39mif\u001b[39;00m retries \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_retry(err\u001b[39m.\u001b[39mresponse):\n\u001b[1;32m    915\u001b[0m     err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 916\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retry_request(\n\u001b[1;32m    917\u001b[0m         options,\n\u001b[1;32m    918\u001b[0m         cast_to,\n\u001b[1;32m    919\u001b[0m         retries,\n\u001b[1;32m    920\u001b[0m         err\u001b[39m.\u001b[39;49mresponse\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    921\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    922\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    925\u001b[0m \u001b[39m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[39m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:958\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[39m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[39m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m    956\u001b[0m time\u001b[39m.\u001b[39msleep(timeout)\n\u001b[0;32m--> 958\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    959\u001b[0m     options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    960\u001b[0m     cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    961\u001b[0m     remaining_retries\u001b[39m=\u001b[39;49mremaining,\n\u001b[1;32m    962\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    963\u001b[0m     stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    964\u001b[0m )\n",
            "File \u001b[0;32m~/Documents/model_host/autogen/.venv/lib/python3.11/site-packages/openai/_base_client.py:930\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mis_closed:\n\u001b[1;32m    928\u001b[0m         err\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mread()\n\u001b[0;32m--> 930\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_status_error_from_response(err\u001b[39m.\u001b[39mresponse) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_response(\n\u001b[1;32m    933\u001b[0m     cast_to\u001b[39m=\u001b[39mcast_to,\n\u001b[1;32m    934\u001b[0m     options\u001b[39m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    937\u001b[0m     stream_cls\u001b[39m=\u001b[39mstream_cls,\n\u001b[1;32m    938\u001b[0m )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-AeXDmWaRtHFPXo4H19CNhKkN on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ],
      "source": [
        "from langchain.tools.file_management.read import ReadFileTool\n",
        "from IPython import get_ipython\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Define a function to generate llm_config from a LangChain tool\n",
        "def generate_llm_config(tool):\n",
        "    # Define the function schema based on the tool's args_schema\n",
        "    function_schema = {\n",
        "        \"name\": tool.name.lower().replace (' ', '_'),\n",
        "        \"description\": tool.description,\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {},\n",
        "            \"required\": [],\n",
        "        },\n",
        "    }\n",
        "\n",
        "    if tool.args is not None:\n",
        "      function_schema[\"parameters\"][\"properties\"] = tool.args\n",
        "\n",
        "    return function_schema\n",
        "\n",
        "def exec_python(cell):\n",
        "    ipython = get_ipython()\n",
        "    result = ipython.run_cell(cell)\n",
        "    log = str(result.result)\n",
        "    if result.error_before_exec is not None:\n",
        "        log += f\"\\n{result.error_before_exec}\"\n",
        "    if result.error_in_exec is not None:\n",
        "        log += f\"\\n{result.error_in_exec}\"\n",
        "    time.sleep(400)\n",
        "    return log\n",
        "\n",
        "def exec_sh(script):\n",
        "    return user_proxy.execute_code_blocks([(\"sh\", script)])\n",
        "\n",
        "\n",
        "# Instantiate the ReadFileTool\n",
        "read_file_tool = ReadFileTool()\n",
        "custom_tool = CircumferenceTool()\n",
        "\n",
        "# Construct the llm_config\n",
        "llm_config = {\n",
        "  #Generate functions config for the Tool\n",
        "  \"functions\":[\n",
        "      generate_llm_config(custom_tool),\n",
        "      generate_llm_config(read_file_tool),\n",
        "      {\n",
        "            \"name\": \"python\",\n",
        "            \"description\": \"run a python script and return the execution result.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"cell\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Valid Python script to execute.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"cell\"],\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"sh\",\n",
        "            \"description\": \"run a shell script and return the execution result.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"script\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Valid shell script to execute.\",\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"script\"],\n",
        "            },\n",
        "        },\n",
        "  ],\n",
        "  \"config_list\": config_list,  # Assuming you have this defined elsewhere\n",
        "  \"timeout\": 600,\n",
        "}\n",
        "\n",
        "user_proxy = autogen.UserProxyAgent(\n",
        "    name=\"user_proxy\",\n",
        "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
        "    human_input_mode=\"NEVER\",\n",
        "    max_consecutive_auto_reply=10,\n",
        "    code_execution_config={\"work_dir\": \"coding\"},\n",
        ")\n",
        "\n",
        "# Register the tool and start the conversation\n",
        "user_proxy.register_function(\n",
        "    function_map={\n",
        "        custom_tool.name: custom_tool._run,\n",
        "        read_file_tool.name: read_file_tool._run,\n",
        "        \"python\":exec_python,\n",
        "        \"sh\":exec_sh,\n",
        "    }\n",
        ")\n",
        "\n",
        "metadata = \"/home/ameya/Documents/model_host/autogen/notebook/file_metadata.json\"\n",
        "\n",
        "def file_metadata(file_path):\n",
        "    with open(file_path, 'r') as json_file:\n",
        "        data = json.load(json_file)\n",
        "        data = json.dumps(data, indent=2)\n",
        "\n",
        "    return str(data)\n",
        "            \n",
        "chatbot = autogen.AssistantAgent(\n",
        "    name=\"chatbot\",\n",
        "    system_message= f\"Reply TERMINATE when the task is done. The string {file_metadata(metadata)} has details of all the files and their description.\",\n",
        "    llm_config=llm_config,\n",
        ")\n",
        "\n",
        "\n",
        "user_proxy.initiate_chat(\n",
        "    chatbot,\n",
        "    message=f\"Go through the folder with the path {get_file_path_of_example()}, and plot a Sinusoidal graph with x and y coordinates.\", #7.81mm in the file\n",
        "    llm_config=llm_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "06e8e3ef",
      "metadata": {},
      "outputs": [],
      "source": [
        "import autogen\n",
        "\n",
        "\n",
        "def describe(file_path):\n",
        "    \n",
        "    config_list = [\n",
        "        {\n",
        "            \"api_key\": \"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\",\n",
        "            \"model\": \"gpt-3.5-turbo\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    llm_config={\n",
        "        \"timeout\": 600,\n",
        "        \"cache_seed\": 44,  # change the seed for different trials\n",
        "        \"config_list\": config_list,\n",
        "        \"temperature\": 0,\n",
        "    }\n",
        "\n",
        "    assistant = autogen.AssistantAgent(\n",
        "        name=\"assistant\",\n",
        "        llm_config=llm_config,\n",
        "        is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
        "    )\n",
        "    # create a UserProxyAgent instance named \"user_proxy\"\n",
        "    user_proxy = autogen.UserProxyAgent(\n",
        "        name=\"user_proxy\",\n",
        "        human_input_mode=\"NEVER\",\n",
        "        is_termination_msg=lambda x: True if \"TERMINATE\" in x.get(\"content\") else False,\n",
        "        max_consecutive_auto_reply=10,\n",
        "        code_execution_config={\n",
        "            \"work_dir\": \"work_dir\",\n",
        "            \"use_docker\": False,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    def extract_code_from_file(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r') as file:\n",
        "                code = file.read()\n",
        "            return code\n",
        "        except FileNotFoundError:\n",
        "            return f\"File not found: {file_path}\"\n",
        "        except Exception as e:\n",
        "            return f\"Error reading file: {e}\"\n",
        "    \n",
        "    code = extract_code_from_file(file_path)\n",
        "\n",
        "    task1 = f\"Describe the code {code} in 2 lines.\"\n",
        "    user_proxy.initiate_chat(assistant, message=task1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5ef47ec",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "Describe the code a = 100\n",
            "b = 22\n",
            "print(a+b)\n",
            " in 2 lines.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "The code you provided is a simple Python script that performs addition of two variables and prints the result. Here is a breakdown of the code:\n",
            "\n",
            "1. `a = 100`: This line assigns the value 100 to the variable `a`.\n",
            "2. `b = 22`: This line assigns the value 22 to the variable `b`.\n",
            "3. `print(a+b)`: This line adds the values of `a` and `b` together and prints the result.\n",
            "\n",
            "So, when you run this code, it will output the sum of `a` and `b`, which is 122.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "The code you provided is already correct. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. There is no need for any changes.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. It seems that I misunderstood your request. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\u001b[33massistant\u001b[0m (to user_proxy):\n",
            "\n",
            "I apologize for the confusion. The code you provided is already correct and does not require any changes. It assigns the value 100 to the variable `a`, assigns the value 22 to the variable `b`, and then prints the sum of `a` and `b`, which is 122. If you have any further questions or need assistance with anything else, please let me know.\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "describe('/home/ameya/Documents/model_host/autogen/test/add2.py')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "flaml_dev",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
